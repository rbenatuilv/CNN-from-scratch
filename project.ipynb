{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto IMT3120\n",
    "## Implementación de Clasificador de Imágenes desde Cero\n",
    "\n",
    "#### Nombre: Roberto Benatuil Valera\n",
    "\n",
    "Este proyecto tendrá como finalidad implementar una red neuronal convolucional para clasificar imágenes. Se buscará implementar las los tipos de capas necesarias, considerando las convolucionales, de pooling, de reshaping y de activación, así como las funciones mismas de activación y de cálculo de pérdida. Cada capa tendrá sus debidos algoritmos de propagación de señales y de gradiente. \n",
    "\n",
    "Al finalizar el modelo, se entrenará con la colección de imágenes del Canadian Institute For Advanced Research, CIFAR10, con el objetivo de comparar su rendimiento con el estado del arte, y analizar su funcionamiento. En este mismo proceso, se realizarán iteraciones y experimentos con diferentes valores de hiperparámetros, entre ellos la tasa de aprendizaje y el tipo y cantidad de capas, así como su tamaño.\n",
    "\n",
    "### Experimentos a realizar\n",
    "\n",
    "Para probar la CNN, se realizarán algunos experimentos, con el fin de analizar su rendimiento y el efecto que los hiperparámetros producen sobre este. Primero, se variará la cantidad de capas convolucionales, cada una con una capa de activación ReLU, con el fin de medir la variación de rendimiento respecto a la eficiencia o rapidez de ejecución.\n",
    "\n",
    "También, se hará otra serie de experimentos en el que se variará la tasa de aprendizaje, que corresponde al nivel de actualización de los parámetros según su gradiente, por cada iteración. Esto, con el fin de medir la velocidad de convergencia y el comportamiento de la curva de error, para encontrar el punto en el que presenta mayor eficiencia.\n",
    "\n",
    "Por último, se intentará hacer una combinación de los resultados anteriores para encontrar el punto de mayor rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Archivos relevantes:\n",
    "\n",
    "`activation.py`: aquí se definen las funciones de activación disponibles.\n",
    "\n",
    "`aux_functions.py`: se definen funciones auxiliares, como pooling, correlación y convolución.\n",
    "\n",
    "`loss_functs.py`: se definen las funciones de pérdida.\n",
    "\n",
    "`nn_layers.py`: se definen las clases de capas de la red neuronal.\n",
    "\n",
    "`nn_class.py`: se define la clase principal de la CNN.\n",
    "\n",
    "`optimization.py`: se crea el optimizador ADAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementación y testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "from nn_class import ConvNeuralNet\n",
    "from aux_functions import preprocess\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 500\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "(x_train, other1, y_train, other2) = train_test_split(x_train, y_train, test_size=0.95, random_state=42, stratify=y_train)\n",
    "(x_test, other1, y_test, other2) = train_test_split(x_test, y_test, test_size=0.95, random_state=42, stratify=y_test)\n",
    "\n",
    "print(len(x_train), len(x_test))\n",
    "\n",
    "x_train, y_train, x_test, y_test, x_val, y_val = preprocess(x_train, y_train, x_test, y_test)\n",
    "\n",
    "x_test = x_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 1\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "1 capa convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting layers: 100%|██████████| 6/6 [00:00<00:00, 3000.22it/s]\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    ('conv', {'kernel_size': 3, 'depth': 3}),\n",
    "    ('maxpool', {'kernel_size': 2}),\n",
    "    ('activation', {'activation': 'relu'}),\n",
    "    ('reshape', {'output_shape': 'flatten'}),\n",
    "    ('dense', {'neurons': 10}),\n",
    "    ('activation', {'activation': 'softmax'})\n",
    "]\n",
    "\n",
    "loss = 'cross_entropy'\n",
    "lr = 0.1\n",
    "\n",
    "input_shape = (3, 32, 32)\n",
    "\n",
    "model1 = ConvNeuralNet(layers, loss, lr, input_shape, name='model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = True\n",
    "if LOAD:\n",
    "    model1.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2000/2000 [00:11<00:00, 172.21it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 247.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train error: 1.9775203252540166, validation error: 2.296474364418621\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2000/2000 [00:12<00:00, 166.17it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 236.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train error: 1.9377569697434405, validation error: 2.2728725097230336\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2000/2000 [00:11<00:00, 170.57it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 212.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train error: 1.8900365363348677, validation error: 2.3557832793196822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2000/2000 [00:13<00:00, 150.40it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 227.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train error: 1.8945391384209864, validation error: 2.4004524425379508\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2000/2000 [00:14<00:00, 139.37it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 169.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train error: 1.8829931355885001, validation error: 2.3692897021426598\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2000/2000 [00:13<00:00, 149.35it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 175.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train error: 1.840966848216205, validation error: 2.5089849844100747\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2000/2000 [00:13<00:00, 144.91it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:02<00:00, 223.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train error: 1.8288689376916454, validation error: 2.446564812410445\n",
      "\n",
      "Early stopping on epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1.fit(x_train, y_train, epochs=20, patience=5, validation_data=(x_val, y_val))\n",
    "model1.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 500/500 [00:02<00:00, 227.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.check_precision(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 2\n",
    "\n",
    "learning_rate = 0.1\n",
    "2 capas convolucionales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting layers: 100%|██████████| 9/9 [00:00<00:00, 4508.93it/s]\n"
     ]
    }
   ],
   "source": [
    "layers = [\n",
    "    ('conv', {'kernel_size': 3, 'depth': 3}),\n",
    "    ('maxpool', {'kernel_size': 2}),\n",
    "    ('activation', {'activation': 'relu'}),\n",
    "    ('conv', {'kernel_size': 2, 'depth': 3}),\n",
    "    ('maxpool', {'kernel_size': 2}),\n",
    "    ('activation', {'activation': 'relu'}),\n",
    "    ('reshape', {'output_shape': 'flatten'}),\n",
    "    ('dense', {'neurons': 10}),\n",
    "    ('activation', {'activation': 'softmax'})\n",
    "]\n",
    "\n",
    "loss = 'cross_entropy'\n",
    "lr = 0.1\n",
    "\n",
    "input_shape = (3, 32, 32)\n",
    "\n",
    "model2 = ConvNeuralNet(layers, loss, lr, input_shape, name='model2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "if LOAD:\n",
    "    model2.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2000/2000 [00:23<00:00, 85.36it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:04<00:00, 106.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 train error: 2.8132263132812954, validation error: 2.1911701436067474\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2000/2000 [00:26<00:00, 74.41it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:05<00:00, 89.29it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 train error: 2.1521722875860405, validation error: 2.1439352430686966\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2000/2000 [00:25<00:00, 77.41it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:05<00:00, 87.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 train error: 2.1168333802385555, validation error: 2.178851220095354\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 2000/2000 [00:26<00:00, 74.73it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:05<00:00, 87.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 train error: 2.106187659981477, validation error: 2.1394491307342514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 2000/2000 [00:29<00:00, 67.98it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:05<00:00, 88.06it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 train error: 2.077516968513714, validation error: 2.273611392642506\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 2000/2000 [00:24<00:00, 81.41it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:04<00:00, 102.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 train error: 2.0973461446205115, validation error: 2.153042799668795\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 2000/2000 [00:26<00:00, 74.94it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:06<00:00, 76.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 train error: 2.0697279942531304, validation error: 2.18787551993399\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 2000/2000 [00:26<00:00, 76.52it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:05<00:00, 98.65it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 train error: 2.0765750375586314, validation error: 2.216037362769548\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 2000/2000 [00:37<00:00, 53.32it/s]\n",
      "Validation: 100%|██████████| 500/500 [00:06<00:00, 77.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 train error: 2.0452007739742513, validation error: 2.182639170924021\n",
      "\n",
      "Early stopping on epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2.fit(x_train, y_train, epochs=20, patience=5, validation_data=(x_val, y_val))\n",
    "model2.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.check_precision(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 3\n",
    "\n",
    "learning_rate = 0.5\n",
    "\n",
    "1 capa convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ('conv', {'kernel_size': 3, 'depth': 3}),\n",
    "    ('maxpool', {'kernel_size': 2}),\n",
    "    ('activation', {'activation': 'relu'}),\n",
    "    ('reshape', {'output_shape': 'flatten'}),\n",
    "    ('dense', {'neurons': 10}),\n",
    "    ('activation', {'activation': 'softmax'})\n",
    "]\n",
    "\n",
    "loss = 'cross_entropy'\n",
    "lr = 0.5\n",
    "\n",
    "input_shape = (3, 32, 32)\n",
    "\n",
    "model3 = ConvNeuralNet(layers, loss, lr, input_shape, name='model3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "if LOAD:\n",
    "    model3.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.fit(x_train, y_train, epochs=20, patience=5, validation_data=(x_val, y_val))\n",
    "model3.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.check_precision(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimento 4\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "1 capa convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [\n",
    "    ('conv', {'kernel_size': 3, 'depth': 3}),\n",
    "    ('maxpool', {'kernel_size': 2}),\n",
    "    ('activation', {'activation': 'relu'}),\n",
    "    ('reshape', {'output_shape': 'flatten'}),\n",
    "    ('dense', {'neurons': 10}),\n",
    "    ('activation', {'activation': 'softmax'})\n",
    "]\n",
    "\n",
    "loss = 'cross_entropy'\n",
    "lr = 0.01\n",
    "\n",
    "input_shape = (3, 32, 32)\n",
    "\n",
    "model4 = ConvNeuralNet(layers, loss, lr, input_shape, name='model4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD = False\n",
    "if LOAD:\n",
    "    model4.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.fit(x_train, y_train, epochs=20, patience=5, validation_data=(x_val, y_val))\n",
    "model4.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4.check_precision(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al contrario de como se esperaba, la red neuronal convolucional no presenta un buen rendimiento. Algunas de las posibles razones pueden ser las siguientes:\n",
    "\n",
    "- Tasa de aprendizaje con mal valor: Una tasa de aprendizaje mala puede ocasionar que el algoritmo de descenso no se comporte de buena manera, es decir, que salte el óptimo y diverja\n",
    "\n",
    "- No se utilizó DropOut: esta técnica permite una regularización de los pesos y evita que el algoritmo quede en mínimos locales. La clase está implementada pero no se utilizó\n",
    "\n",
    "- Utilización de algoritmo ADAM: Este algoritmo hace algo similar a lo anterior, y se implementó mas no se aplicó directamente.\n",
    "\n",
    "Se buscará seguir estudiando el comportamiento de la CNN para su correcto funcionamiento a futuro.\n",
    "\n",
    "Repo de GitHub: https://github.com/rbenatuilv/CNN-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
